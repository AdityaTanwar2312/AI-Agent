from transformers import AutoModel, AutoTokenizer
import torch

batch_size = 32
block_size = 8

with open("input.txt", "r", encoding="utf-8") as f:
    text = f.read()

# Generate embeddings for the data
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")


token_size = 512
max_length = 0
embeddings = torch.tensor([])
while max_length < text.__len__():
    inputs = tokenizer(
        text[max_length-512:max_length],
        max_length=max_length,
        truncation=True,  # Ensures input is truncated to 512 tokens
        return_tensors="pt"
    )
    outputs = model(**inputs)
    result = outputs.last_hidden_state.mean(dim=1)
    embeddings = torch.concat((embeddings, result), dim=1)
    max_length = max_length + 512 

print(embeddings.shape)

# Normalize the embeddings generated by the hugging-face model

data = embeddings
ix = torch.randint(len(data) - block_size, (batch_size,))
x = torch.stack([data[i:i+block_size] for i in ix])